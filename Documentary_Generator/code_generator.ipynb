{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Teywa-OZIOL/AI_Content_Factory/blob/main/Documentary_Generator/code_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function Installation"
      ],
      "metadata": {
        "id": "c3U5hepT4XTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google.genai\n",
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkfwY-in4idX",
        "outputId": "794777bc-f905-4337-e296-8ec73aa022cb"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google.genai in /usr/local/lib/python3.12/dist-packages (1.46.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google.genai) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google.genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google.genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google.genai) (2.11.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google.genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google.genai) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google.genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google.genai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google.genai) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google.genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google.genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google.genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google.genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google.genai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google.genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google.genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google.genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google.genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google.genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google.genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google.genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google.genai) (0.6.1)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.33.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Package"
      ],
      "metadata": {
        "id": "H2JqfjGT4dqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "import wave\n",
        "from google.colab import userdata\n",
        "from groq import Groq\n",
        "import json\n",
        "import re"
      ],
      "metadata": {
        "id": "VtQpWtjM94Ls"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Parameters"
      ],
      "metadata": {
        "id": "5dRjES5c__HH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC = \"La Révolution française (1789)\"\n",
        "FILENAME = re.sub(r'[^a-zA-Z0-9_-]+', '_', TOPIC.lower())\n",
        "OUTPUT_DIR = \"outputs\"\n",
        "FORMAT = \"MEDIUM\" # LONG (20 minutes) / MEDIUM (8 minutes) / SHORT (1 minute)\n",
        "TYPE_DOC = 'DOCUMENTARY' # STORY / PODCAST"
      ],
      "metadata": {
        "id": "oHvR_LypAHnS"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir outputs"
      ],
      "metadata": {
        "id": "lGys287UQBxK",
        "outputId": "ea3743b6-c2e7-4e9d-fdc8-02d806bc7eb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘outputs2’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 : Plan Generation"
      ],
      "metadata": {
        "id": "HmFfVKU87pLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_output(text):\n",
        "    cleaned = text.strip(\"```python\").strip(\"```\").strip()\n",
        "    try:\n",
        "        return eval(cleaned)\n",
        "    except Exception:\n",
        "        try:\n",
        "            return json.loads(cleaned)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "def topic_analyzer(topic, format, type_doc):\n",
        "    GROQ_API_KEY = userdata.get('GROQ')\n",
        "    client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "    if format == \"LONG\":\n",
        "        nb_chapters = \"5\"\n",
        "    elif format == \"MEDIUM\":\n",
        "        nb_chapters = \"4\"\n",
        "    elif format == \"SHORT\":\n",
        "        nb_chapters = \"3\"\n",
        "    else:\n",
        "      raise ValueError(f\"Invalid format: {format}\")\n",
        "\n",
        "    if type_doc == \"STORY\":\n",
        "        type_content = \"histoire\"\n",
        "    elif type_doc == \"PODCAST\":\n",
        "        type_content = \"podcast\"\n",
        "    elif type_doc == \"DOCUMENTARY\":\n",
        "        type_content = \"documentaire\"\n",
        "    else:\n",
        "      raise ValueError(f\"Invalid type_doc: {type_doc}\")\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "        Tu es un expert en storytelling {type_content} et en référencement YouTube (SEO).\n",
        "        Ton rôle :\n",
        "        - Créer des plans de {type_content} captivants et informatifs.\n",
        "        - Générer des titres accrocheurs (type YouTube) et des descriptions SEO de 150–250 mots.\n",
        "        - Employer un ton immersif, pédagogique et crédible.\n",
        "        - Utiliser des mots-clés pertinents pour le référencement SEO.\n",
        "        - Ne dis pas que le {type_content} est basé sur des images d'archives, des analyses expertes,\n",
        "          c'est plutot un récit, une narration\n",
        "        - Toujours respecter strictement le format demandé (dictionnaire Python).\n",
        "        - Je veux exactement {nb_chapters} chapitres en plus de l'introduction et de la conclusion.\n",
        "    \"\"\"\n",
        "\n",
        "    if type_doc in ['STROY', 'DOCUMENTARY']:\n",
        "        example = \"\"\"Exemple de format attendu :\n",
        "            {\n",
        "              \"titre_video\": \"La Guerre Froide : Quand le monde a failli exploser\",\n",
        "              \"description_SEO\": \"Découvrez l’histoire fascinante de la Guerre Froide...\",\n",
        "              \"mots_cles\": [\"guerre froide\", \"URSS\", \"États-Unis\", \"crise de Cuba\"],\n",
        "              \"chapitres\": [\n",
        "                {\"titre\": \"1. Le monde après 1945\", \"contenu\": \"Après la Seconde Guerre mondiale...\"},\n",
        "                {\"titre\": \"2. Le rideau de fer\", \"contenu\": \"La division de l’Europe en deux blocs...\"},\n",
        "                ...\n",
        "              ]\n",
        "            }\n",
        "        \"\"\"\n",
        "    else:\n",
        "        # TO DO PODCAST MODE\n",
        "        print(\"PODCAST mode not ready\")\n",
        "        return\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "        Crée un plan complet de {type_content} sur : \"{topic}\".\n",
        "\n",
        "        Respecte exactement la structure suivante :\n",
        "        {{\n",
        "          \"titre_video\": str,          # titre accrocheur, format YouTube\n",
        "          \"description_SEO\": str,      # 150–250 mots, riche en mots-clés, ton narratif\n",
        "          \"mots_cles\": [str, ...],     # mots-clés pertinents pour le SEO\n",
        "          \"chapitres\": [               # 3 à 5 chapitres, chacun 3 à 5 phrases\n",
        "            {{\"titre\": str, \"contenu\": str}},\n",
        "            ...\n",
        "          ]\n",
        "        }}\n",
        "\n",
        "        {example}\n",
        "\n",
        "        Règles :\n",
        "        - Retourne UNIQUEMENT le dictionnaire Python, sans texte autour.\n",
        "        - Le ton doit être immersif et dynamique.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=2500,\n",
        "    )\n",
        "\n",
        "    raw_output = response.choices[0].message.content.strip()\n",
        "\n",
        "    plan = parse_output(raw_output)\n",
        "\n",
        "    output_path = f\"{OUTPUT_DIR}/PART1_{FILENAME}.json\"\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(plan, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print('===============================================')\n",
        "    print(f\"✅ PART 1 : Documentary Plan Generated\")\n",
        "    print('===============================================')\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "V6g_G4Wt-6lf"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 : Script Generation"
      ],
      "metadata": {
        "id": "FoXndRJY9sEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(client, prompt, max_tokens=1500, temperature=0.7):\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "        )\n",
        "        text = response.choices[0].message.content.strip()\n",
        "        return text.strip(\"```\").strip(\"python\").strip()\n",
        "\n",
        "def script_generator(topic, format, type_doc):\n",
        "\n",
        "    GROQ_API_KEY = userdata.get('GROQ')\n",
        "    client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "    plan_path = f\"{OUTPUT_DIR}/PART1_{FILENAME}.json\"\n",
        "\n",
        "    with open(plan_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        plan = json.load(f)\n",
        "\n",
        "    topic = plan[\"titre_video\"]\n",
        "\n",
        "    output_json_path = f\"{OUTPUT_DIR}/PART2_{FILENAME}.json\"\n",
        "\n",
        "    script_chapters = []\n",
        "\n",
        "    if format == \"LONG\":\n",
        "        name_document = \"Documentaire\"\n",
        "        nb_mots_intro_conclu = \"250\"\n",
        "        nb_mots_chapitre = \"350\"\n",
        "    elif format == \"MEDIUM\":\n",
        "        name_document = \"Court Documentaire\"\n",
        "        nb_mots_intro_conclu = \"125\"\n",
        "        nb_mots_chapitre = \"175\"\n",
        "    elif format == \"SHORT\":\n",
        "        name_document = \"Mini Documentaire\"\n",
        "        nb_mots_intro_conclu = \"50\"\n",
        "        nb_mots_chapitre = \"80\"\n",
        "    else:\n",
        "      raise ValueError(f\"Invalid format: {format}\")\n",
        "\n",
        "    intro_prompt = f\"\"\"\n",
        "        Rédige une introduction captivante pour le {name_document} '{topic}'.\n",
        "        Durée : ~{nb_mots_intro_conclu} mots.\n",
        "        Style : narratif immersif, comme une voix-off de Netflix/Arte.\n",
        "        Ne mentionne aucun chapitre encore.\n",
        "        Ajoute entre crochet, la manière dont il faut raconter certaines phrases entre colère, joie, tristesse, résignation, ...\n",
        "        Ces crochets sont placés impérativement avant le début de la phrase et il ne peut pas y avoir deux crochets consécutifs.\n",
        "        Exemple : [Voix basse et grave] [Joie, élévation de voix] [rire] [Voix ému] [avec exclamation] [Soupire]\n",
        "        Aucun texte entre parenthèse, seulement des crochets.\n",
        "    \"\"\"\n",
        "\n",
        "    intro_text = generate_text(client, intro_prompt)\n",
        "\n",
        "    script_chapters.append({\n",
        "        \"numero\": 0,\n",
        "        \"titre\": \"Introduction\",\n",
        "        \"texte\": intro_text\n",
        "    })\n",
        "\n",
        "    print(\"✅ PART 2 : Introduction Script Generated\")\n",
        "\n",
        "    previous_context = intro_text\n",
        "    for idx, chap in enumerate(plan[\"chapitres\"], start=1):\n",
        "        chap_prompt = f\"\"\"\n",
        "            Tu écris le chapitre {idx} d'un documentaire sur '{topic}'.\n",
        "            Titre du chapitre : \"{chap['titre']}\"\n",
        "            Résumé du plan : \"{chap['contenu']}\"\n",
        "            Contexte narratif précédent : \"{previous_context}\"\n",
        "\n",
        "            Écris un texte narratif immersif, détaillé (~{nb_mots_chapitre} mots minimum), fluide et captivant.\n",
        "            Inclure anecdotes, dates, personnages et transitions.\n",
        "            Ne contredis aucun texte précédent.\n",
        "            Ajoute entre crochet, la manière dont il faut raconter certaines phrases entre colère, joie, tristesse, résignation, ...\n",
        "            Ces crochets sont placés impérativement avant le début de la phrase et il ne peut pas y avoir deux crochets consécutifs.\n",
        "            Exemple : [Voix basse et grave] [Joie, élévation de voix] [rire] [Voix ému] [avec exclamation] [Soupire]\n",
        "            Aucun texte entre parenthèse, seulement des crochets.\n",
        "        \"\"\"\n",
        "        chap_text = generate_text(client, chap_prompt)\n",
        "        script_chapters.append({\n",
        "            \"numero\": idx,\n",
        "            \"titre\": chap[\"titre\"],\n",
        "            \"texte\": chap_text\n",
        "        })\n",
        "        previous_context += \" \" + chap_text\n",
        "\n",
        "        print(f\"✅ PART 2 : Chapter {idx} Script Generated\")\n",
        "\n",
        "    conclusion_prompt = f\"\"\"\n",
        "        Rédige une conclusion forte et mémorable pour le documentaire '{topic}'.\n",
        "        Contexte narratif précédent : \"{previous_context}\"\n",
        "        Durée : ~{nb_mots_intro_conclu} mots.\n",
        "        Résume l'ensemble, relie les chapitres et termine sur une phrase marquante.\n",
        "        Ajoute entre crochet, la manière dont il faut raconter certaines phrases entre colère, joie, tristesse, résignation, ...\n",
        "        Ces crochets sont placés impérativement avant le début de la phrase et il ne peut pas y avoir deux crochets consécutifs.\n",
        "        Exemple : [Voix basse et grave] [Joie, élévation de voix] [rire] [Voix ému] [avec exclamation] [Soupire]\n",
        "        Aucun texte entre parenthèse, seulement des crochets.\n",
        "    \"\"\"\n",
        "\n",
        "    conclusion_text = generate_text(client, conclusion_prompt)\n",
        "\n",
        "    script_chapters.append({\n",
        "        \"numero\": len(plan[\"chapitres\"]) + 1,\n",
        "        \"titre\": \"Conclusion\",\n",
        "        \"texte\": conclusion_text\n",
        "    })\n",
        "    print(\"✅ PART 2 : Conclusion Script Generated\")\n",
        "\n",
        "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(script_chapters, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print('===============================================')\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "q2hAeLrF-n0I"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 : Audio Generation"
      ],
      "metadata": {
        "id": "dXbZOoKJ9wv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wave_file(filename, pcm, channels=1, rate=24000, sample_width=2):\n",
        "  with wave.open(filename, \"wb\") as wf:\n",
        "      wf.setnchannels(channels)\n",
        "      wf.setsampwidth(sample_width)\n",
        "      wf.setframerate(rate)\n",
        "      wf.writeframes(pcm)\n",
        "  return\n",
        "\n",
        "def generate_audio(client, prompt, filename):\n",
        "  voice_name = \"Iapetus\"\n",
        "  MODEL_ID = \"gemini-2.5-flash-preview-tts\"\n",
        "  response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config={\"response_modalities\": ['Audio'],\n",
        "            \"speech_config\": {\"voice_config\": {\"prebuilt_voice_config\": {\"voice_name\": voice_name}}}},\n",
        "    )\n",
        "  data = response.candidates[0].content.parts[0].inline_data.data\n",
        "  wave_file(filename, data)\n",
        "  return\n",
        "\n",
        "def voice_generation():\n",
        "  GOOGLE_API_KEY = userdata.get('GEMINI')\n",
        "  client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "  script_path = f\"{OUTPUT_DIR}/PART2_{FILENAME}.json\"\n",
        "  with open(script_path, \"r\", encoding=\"utf-8\") as f:\n",
        "      text_script = json.load(f)\n",
        "\n",
        "  for text in text_script:\n",
        "      text_to_read = text[\"texte\"]\n",
        "      prompt = f\"\"\"\n",
        "          Read this in French with the voice of an old sage recounting the war:\n",
        "          It takes passion, emotion, feelings. The speech rate should be slightly fast.\n",
        "          At the beginning of each sentence, you have the manner and intonation with which the sentence should be read in brackets.\n",
        "          You must not read what is between the brackets\n",
        "          Here the text :\n",
        "          {text_to_read}\n",
        "      \"\"\"\n",
        "      generate_audio(client, prompt, f\"{OUTPUT_DIR}/audio_{text['numero']}.wav\")\n",
        "      print(f\"✅ PART 3 : {text['numero']} Script Generated\")\n",
        "\n",
        "  print('===============================================')\n",
        "  return"
      ],
      "metadata": {
        "id": "suOCpDMk6QUe"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN"
      ],
      "metadata": {
        "id": "rJF6i-LcBYn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(topic, format, type_doc):\n",
        "\n",
        "    topic_analyzer(topic, format, type_doc)\n",
        "    script_generator(topic, format, type_doc)\n",
        "    voice_generation()\n",
        "    #image_generation()\n",
        "    #video_generation()\n",
        "    #music_generation()\n",
        "    #video_editing()\n",
        "\n",
        "    return\n",
        "\n",
        "main(TOPIC, FORMAT, TYPE_DOC)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "KbwFiWUO-1QX",
        "outputId": "6748a71f-af68-4465-bfbd-40cdaf60f855"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============================================\n",
            "✅ PART 1 : Documentary Plan Generated\n",
            "===============================================\n",
            "✅ PART 2 : Introduction Script Generated\n",
            "✅ PART 2 : Chapter 1 Script Generated\n",
            "✅ PART 2 : Chapter 2 Script Generated\n",
            "✅ PART 2 : Chapter 3 Script Generated\n",
            "✅ PART 2 : Chapter 4 Script Generated\n",
            "✅ PART 2 : Chapter 5 Script Generated\n",
            "✅ PART 2 : Chapter 6 Script Generated\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k8p83668e5qb4pwpb0a2zcz3` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99478, Requested 5747. Please try again in 1h15m14.4s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1880038892.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTOPIC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFORMAT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTYPE_DOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1880038892.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(topic, format, type_doc)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtopic_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mscript_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvoice_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#image_generation()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1087925032.py\u001b[0m in \u001b[0;36mscript_generator\u001b[0;34m(topic, format, type_doc)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \"\"\"\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mconclusion_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconclusion_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     script_chapters.append({\n",
            "\u001b[0;32m/tmp/ipython-input-1087925032.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(client, prompt, max_tokens, temperature)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m         response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama-3.3-70b-versatile\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    462\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \"\"\"\n\u001b[0;32m--> 464\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k8p83668e5qb4pwpb0a2zcz3` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99478, Requested 5747. Please try again in 1h15m14.4s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#files.download(\"mon_fichier.csv\")"
      ],
      "metadata": {
        "id": "vCb3nMRb_0Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "zip_filename = f\"{OUTPUT_DIR}.zip\"\n",
        "shutil.make_archive(OUTPUT_DIR, 'zip', OUTPUT_DIR)\n",
        "files.download(zip_filename)"
      ],
      "metadata": {
        "id": "aiNPSKjADAOn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0fgOxpmGrOvn"
      ],
      "name": "Get_started_TTS.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}